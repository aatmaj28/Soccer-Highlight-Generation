{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b32a0b-0c64-41b6-88d1-6b3e92075ac6",
   "metadata": {},
   "source": [
    "# Split the Big Video and Store in their corresponding folders on the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d28126-7a27-4a59-826e-6d03bedc0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Rankings of event labels\n",
    "RANKINGS = {\n",
    "    \"Kick-off\": 1, \"Goal\": 2, \"Shots on target\": 3, \"Red card\": 4, \"Corner\": 5,\n",
    "    \"Yellow card\": 6, \"Shots off target\": 7, \"Foul\": 8, \"Direct free-kick\": 9,\n",
    "    \"Offside\": 10, \"Clearance\": 11, \"Indirect free-kick\": 12, \"Throw-in\": 13,\n",
    "    \"Ball out of play\": 14, \"Substitution\": 15\n",
    "}\n",
    "\n",
    "def extract_events_with_context(data, context_time=10000):\n",
    "    \"\"\"Extracts events with surrounding context.\"\"\"\n",
    "    events = []\n",
    "    \n",
    "    for idx, annotation in enumerate(data[\"annotations\"]):\n",
    "        # Ensure 'position' is treated as an integer\n",
    "        try:\n",
    "            event_position = int(annotation[\"position\"])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Invalid position value {annotation['position']} at index {idx}. Skipping this event.\")\n",
    "            continue\n",
    "        \n",
    "        start_time = max(0, event_position - context_time)\n",
    "        end_time = event_position + context_time\n",
    "\n",
    "        events.append({\n",
    "            \"label\": annotation[\"label\"],\n",
    "            \"event_position\": event_position,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"gameTime\": annotation[\"gameTime\"],\n",
    "            \"index\": idx\n",
    "        })\n",
    "\n",
    "    return events\n",
    "\n",
    "def extract_frames(video_path, start_time, end_time, frame_rate=30):\n",
    "    \"\"\"Extracts frames from the video within the given time range.\"\"\"\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video.set(cv2.CAP_PROP_POS_MSEC, start_time)\n",
    "    frames = []\n",
    "    for _ in range(int((end_time - start_time) / 1000 * frame_rate)):\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "def save_video(frames, output_path, frame_rate=30):\n",
    "    \"\"\"Creates a video from frames.\"\"\"\n",
    "    if not frames:\n",
    "        print(f\"No frames to create video at {output_path}.\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = frames[0].shape\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        writer.write(frame)\n",
    "\n",
    "    writer.release()\n",
    "    print(f\"Video saved at {output_path}\")\n",
    "\n",
    "def process_match(json_file_path, video_dir, output_dir, context_time=10000):\n",
    "    \"\"\"Processes a single match, extracting event clips.\"\"\"\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"No JSON file found at {json_file_path}.\")\n",
    "        return\n",
    "\n",
    "    with open(json_file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    events = extract_events_with_context(data, context_time)\n",
    "\n",
    "    for event in events:\n",
    "        event_label = event[\"label\"]\n",
    "        half = event[\"gameTime\"][0]\n",
    "        video_path = os.path.join(video_dir, f\"{half}_720p.mkv\")\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: Video file {video_path} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        label_folder = event_label if RANKINGS.get(event_label, 15) <= 10 else \"nothing\"\n",
    "        label_dir = os.path.join(output_dir, label_folder)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        frames = extract_frames(video_path, event[\"start_time\"], event[\"end_time\"])\n",
    "        output_video = os.path.join(label_dir, f\"{label_folder}_half_{half}_pos_{event['event_position']}_idx_{event['index']}.mp4\")\n",
    "        save_video(frames, output_video)\n",
    "\n",
    "def process_directory(json_file_path, video_dir, output_dir):\n",
    "    \"\"\"Processes videos based on a JSON file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Processing videos in: {video_dir}\")\n",
    "    process_match(json_file_path, video_dir, output_dir)\n",
    "\n",
    "# Example paths for JSON file, video directory, and output directory\n",
    "json_file_path = \"Labels-v2.json\"\n",
    "video_dir = \"D:\\\\FAI Project\\\\FAI_Data_Final\\\\Full_Vids_2\"\n",
    "output_dir = \"D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted2\"\n",
    "\n",
    "# Process videos\n",
    "process_directory(json_file_path, video_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49281f8-5b6c-465f-8c79-4cbf3301b777",
   "metadata": {},
   "source": [
    "# Access the \"extracted\" folder, then create .pt files for Training and Testing (Video + Label pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a5600-3e8a-465b-9cd8-10300ebd41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If CUDA is available, print additional GPU information\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "# Load the ResNet model and move it to the GPU\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# Function to extract features from frames of a video clip\n",
    "def extract_features(video_clip):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_clip)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "    feature_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for frame in frames:\n",
    "            # Move input tensor to GPU\n",
    "            input_tensor = torch.tensor(frame).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
    "            feature = resnet(input_tensor)\n",
    "            # Move feature back to CPU for numpy conversion\n",
    "            feature_vectors.append(feature.cpu().numpy())\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "# Load Label annotations from folder names\n",
    "def get_label_from_folder(folder_name):\n",
    "    return folder_name\n",
    "\n",
    "# Specify the root directory containing the extracted folders\n",
    "root_directory = r'D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted'\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate through each folder inside the extracted directory\n",
    "for folder_name in tqdm(os.listdir(root_directory), desc=\"Processing folders\"):\n",
    "    folder_path = os.path.join(root_directory, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Ensure it is a folder\n",
    "        # Use folder name as the label\n",
    "        label = get_label_from_folder(folder_name)\n",
    "        \n",
    "        # Iterate through video files in the folder\n",
    "        video_files = [f for f in os.listdir(folder_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing videos in {folder_name}\", leave=False):\n",
    "            video_clip_path = os.path.join(folder_path, video_file)\n",
    "            features = extract_features(video_clip_path)\n",
    "            \n",
    "            # Collect all features and their corresponding labels\n",
    "            all_features.append(features)\n",
    "            all_labels.extend([label] * features.shape[0])  # Add the label for each frame\n",
    "\n",
    "# Convert features to numpy array\n",
    "all_features = np.concatenate(all_features)\n",
    "\n",
    "# Create a mapping from labels to integers\n",
    "unique_labels = list(set(all_labels))\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "print(\"Mapping from labels to integers:\", label_to_index)\n",
    "\n",
    "# Convert all_labels to integers\n",
    "numeric_labels = [label_to_index[label] for label in all_labels]\n",
    "\n",
    "# Split features and labels into training and testing sets (80:20 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features, \n",
    "    numeric_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Calculate the number of unique classes for the output layer\n",
    "output_size = len(unique_labels)\n",
    "print(\"Feature extraction completed.\")\n",
    "\n",
    "# Paths for saving training and testing data\n",
    "train_save_path = r'D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted_features1.pt'\n",
    "test_save_path = r'D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted_features2.pt'\n",
    "\n",
    "# Save training features and labels\n",
    "torch.save({\n",
    "    'features': X_train,\n",
    "    'labels': y_train,\n",
    "    'label_to_index': label_to_index,\n",
    "    'unique_labels': unique_labels\n",
    "}, train_save_path)\n",
    "\n",
    "# Save testing features and labels\n",
    "torch.save({\n",
    "    'features': X_test,\n",
    "    'labels': y_test,\n",
    "    'label_to_index': label_to_index,\n",
    "    'unique_labels': unique_labels\n",
    "}, test_save_path)\n",
    "\n",
    "print(f\"Training features and labels saved to {train_save_path}\")\n",
    "print(f\"Testing features and labels saved to {test_save_path}\")\n",
    "\n",
    "# Print some additional information about the split\n",
    "print(f\"Total number of samples: {len(numeric_labels)}\")\n",
    "print(f\"Number of training samples: {len(y_train)}\")\n",
    "print(f\"Number of testing samples: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa955d-3af8-408d-bee2-f1de47280acc",
   "metadata": {},
   "source": [
    "# Training Loop using the .pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61758753-a277-425f-a688-c2c07f0cf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define the device based on CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define an enhanced GRU model with more layers and dropout\n",
    "class EnhancedGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.3):\n",
    "        super(EnhancedGRUModel, self).__init__()\n",
    "        \n",
    "        # Add multiple GRU layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layers with increased hidden size\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)  # Increase the size of the first FC layer\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)  # Keep the second layer the same size\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # Final output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Apply dropout again\n",
    "        return self.fc3(out)\n",
    "\n",
    "# Load the previously extracted features\n",
    "features_path = r'D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted_features1.pt'\n",
    "loaded_data = torch.load(features_path)\n",
    "\n",
    "# Extract features and labels\n",
    "all_features = loaded_data['features']\n",
    "all_labels = loaded_data['labels']\n",
    "label_to_index = loaded_data['label_to_index']\n",
    "unique_labels = loaded_data['unique_labels']\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(\"Loaded Features Shape:\", all_features.shape)\n",
    "print(\"Loaded Labels Shape:\", all_labels.shape)\n",
    "print(\"Unique Labels:\", unique_labels)\n",
    "print(\"Label to Index Mapping:\", label_to_index)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = all_features.shape[2]  # Feature vector size\n",
    "hidden_size = 2048  # Increased hidden size for better representation\n",
    "output_size = len(unique_labels)  # Set output size based on the number of unique labels\n",
    "num_epochs = 50  # Increased epochs for potentially better training\n",
    "batch_size = 16  # Adjusted batch size\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = TensorDataset(all_features, all_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = EnhancedGRUModel(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with validation tracking\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for features_batch, labels_batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        features_batch, labels_batch = features_batch.to(device), labels_batch.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'label_to_index': label_to_index,\n",
    "            'unique_labels': unique_labels,\n",
    "            'input_size': input_size,  # Add this\n",
    "            'output_size': output_size  # Add this\n",
    "        }, 'best_enhanced_gru_model.pth')\n",
    "        print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "# Final model save\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': best_loss,\n",
    "    'label_to_index': label_to_index,\n",
    "    'unique_labels': unique_labels,\n",
    "    'input_size': input_size,  # Add this\n",
    "    'output_size': output_size  # Add this\n",
    "}, 'best_enhanced_gru_model.pth')\n",
    "\n",
    "print(\"Training completed. Models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f90f09-40d3-43e0-ad7b-e4e9b9fbfc5a",
   "metadata": {},
   "source": [
    "# Calculating the Accuracy and other metrics on all the catergories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0e95c-77e6-4f41-adda-9e88715da536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the same GRU model architecture\n",
    "class EnhancedGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.3):\n",
    "        super(EnhancedGRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the features\n",
    "features_path = r'D:\\\\FAI Project\\\\FAI_Data_Final\\\\extracted_features1.pt'\n",
    "loaded_data = torch.load(features_path)\n",
    "\n",
    "# Extract features and labels\n",
    "all_features = loaded_data['features']\n",
    "all_labels = loaded_data['labels']\n",
    "label_to_index = loaded_data['label_to_index']\n",
    "unique_labels = loaded_data['unique_labels']\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = TensorDataset(all_features, all_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load the best model\n",
    "model_path = 'best_enhanced_gru_model.pth'\n",
    "checkpoint = torch.load(model_path)\n",
    "model = EnhancedGRUModel(\n",
    "    input_size=all_features.shape[2], \n",
    "    hidden_size=2048, \n",
    "    output_size=len(unique_labels)\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Evaluation\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "class_correct = {label: 0 for label in unique_labels}\n",
    "class_total = {label: 0 for label in unique_labels}\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = torch.zeros(len(unique_labels), len(unique_labels), dtype=torch.int32)\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for features_batch, labels_batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        features_batch, labels_batch = features_batch.to(device), labels_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features_batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update total predictions\n",
    "        total_predictions += labels_batch.size(0)\n",
    "        correct_predictions += (predicted == labels_batch).sum().item()\n",
    "        \n",
    "        # Update class-wise accuracy\n",
    "        for t, p in zip(labels_batch, predicted):\n",
    "            # Update confusion matrix\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "            \n",
    "            # Class-wise accuracy\n",
    "            if t == p:\n",
    "                class_correct[unique_labels[t]] += 1\n",
    "            class_total[unique_labels[t]] += 1\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n",
    "\n",
    "# Print class-wise accuracy\n",
    "print(\"\\nClass-wise Accuracy:\")\n",
    "for label in unique_labels:\n",
    "    class_acc = 100 * class_correct[label] / class_total[label] if class_total[label] > 0 else 0\n",
    "    print(f\"{label}: {class_acc:.2f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"Rows: True Labels, Columns: Predicted Labels\")\n",
    "print(confusion_matrix.numpy())\n",
    "\n",
    "# Optional: Visualize confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix.numpy(), \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            xticklabels=unique_labels, \n",
    "            yticklabels=unique_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df9f66-47c6-4b9d-b71d-23fd3c089831",
   "metadata": {},
   "source": [
    "# Predition on a Single Test video using the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0716e-9e72-4b73-bee6-8ea9186258ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Flexible GRU Model with configurable input size\n",
    "class FlexibleGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.3):\n",
    "        super(FlexibleGRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "def preprocess_video(video_path, target_frames=100, target_size=(224, 224), feature_reduction=True):\n",
    "    \"\"\"\n",
    "    Preprocess video with optional feature reduction\n",
    "    \"\"\"\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Read frames\n",
    "    frames = []\n",
    "    while len(frames) < target_frames and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize frame\n",
    "        frame_resized = cv2.resize(frame, target_size)\n",
    "        \n",
    "        # Convert to float and normalize\n",
    "        frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
    "        frames.append(frame_normalized)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad or truncate to target_frames\n",
    "    if len(frames) < target_frames:\n",
    "        padding = [np.zeros_like(frames[0])] * (target_frames - len(frames))\n",
    "        frames.extend(padding)\n",
    "    elif len(frames) > target_frames:\n",
    "        indices = np.linspace(0, len(frames) - 1, target_frames).astype(int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    video_tensor = np.array(frames)\n",
    "    \n",
    "    # Optional feature reduction\n",
    "    if feature_reduction:\n",
    "        # Reduce features by averaging color channels or using PCA\n",
    "        feature_vector = video_tensor.reshape(target_frames, -1)\n",
    "        \n",
    "        # You can add more advanced feature reduction here if needed\n",
    "        # For example, using PCA or other dimensionality reduction techniques\n",
    "        \n",
    "        return feature_vector\n",
    "    \n",
    "    # If no reduction, return full tensor\n",
    "    return video_tensor.reshape(target_frames, -1)\n",
    "\n",
    "def extract_features(video_path, target_feature_size=1000):\n",
    "    \"\"\"\n",
    "    Extract features from the video with optional size reduction\n",
    "    \"\"\"\n",
    "    # Extract full features\n",
    "    full_features = preprocess_video(video_path)\n",
    "    \n",
    "    # If features are larger than target, reduce using averaging\n",
    "    if full_features.shape[1] > target_feature_size:\n",
    "        # Reduce features by averaging or sampling\n",
    "        feature_indices = np.linspace(0, full_features.shape[1]-1, target_feature_size).astype(int)\n",
    "        reduced_features = full_features[:, feature_indices]\n",
    "        return torch.FloatTensor(reduced_features).unsqueeze(0)\n",
    "    \n",
    "    # If features are smaller, pad with zeros\n",
    "    elif full_features.shape[1] < target_feature_size:\n",
    "        padded_features = np.zeros((full_features.shape[0], target_feature_size))\n",
    "        padded_features[:, :full_features.shape[1]] = full_features\n",
    "        return torch.FloatTensor(padded_features).unsqueeze(0)\n",
    "    \n",
    "    # If features match exactly\n",
    "    return torch.FloatTensor(full_features).unsqueeze(0)\n",
    "\n",
    "def predict_video_label(model, features, label_to_index, unique_labels):\n",
    "    \"\"\"\n",
    "    Predict the label for a video using the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = features.to(next(model.parameters()).device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_index = predicted.item()\n",
    "        predicted_label = unique_labels[predicted_index]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "def main(video_path, model_path):\n",
    "    \"\"\"\n",
    "    Main function to load model and predict video label\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the saved model with weights_only=False for full compatibility\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Retrieve necessary information from checkpoint\n",
    "    label_to_index = checkpoint['label_to_index']\n",
    "    unique_labels = checkpoint['unique_labels']\n",
    "    \n",
    "    # Set up model parameters\n",
    "    hidden_size = 2048\n",
    "    output_size = len(unique_labels)\n",
    "    \n",
    "    # Try to get the original input size from the checkpoint if possible\n",
    "    input_size = checkpoint.get('input_size', 1000)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FlexibleGRUModel(input_size, hidden_size, output_size).to(device)\n",
    "    \n",
    "    # Attempt to load model weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Extract features from the video\n",
    "    video_features = extract_features(video_path, target_feature_size=input_size)\n",
    "    \n",
    "    # Predict label\n",
    "    predicted_label = predict_video_label(model, video_features, label_to_index, unique_labels)\n",
    "    \n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = r\"Corner_half_1_pos_2545244_idx_92.mp4\"\n",
    "    model_path = 'best_enhanced_gru_model.pth'\n",
    "    \n",
    "    main(video_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ad780-7a29-4dd4-9915-f1dc628d0feb",
   "metadata": {},
   "source": [
    "# Final Stitching Logic by taking the Big Video as the input (Options - 3/5 min highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37079735-2c49-4583-88e0-604db1d66394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "class EnhancedGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.3):\n",
    "        super(EnhancedGRUModel, self).__init__()\n",
    "        \n",
    "        self.feature_reducer = nn.Linear(input_size, 1000)  # Reduce features to 1000\n",
    "        \n",
    "        self.gru = nn.GRU(1000, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reduce feature dimensions\n",
    "        x = self.feature_reducer(x)\n",
    "        \n",
    "        out, _ = self.gru(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "class HighlightGenerator:\n",
    "    def __init__(self, model_path='best_enhanced_gru_model.pth'):\n",
    "        # Rankings dictionary\n",
    "        self.rankings = {\n",
    "            \"Kick-off\": 1, \"Goal\": 2, \"Shots on target\": 3, \"Red card\": 4, \n",
    "            \"Corner\": 5, \"Yellow card\": 6, \"Shots off target\": 7, \n",
    "            \"Foul\": 8, \"Direct free-kick\": 9, \"Offside\": 10, \"nothing\": 11\n",
    "        }\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load the model with weights_only=True\n",
    "        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
    "        \n",
    "        # Model parameters\n",
    "        input_size = 2048  # ResNet feature size\n",
    "        hidden_size = 2048\n",
    "        output_size = checkpoint['output_size']\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = EnhancedGRUModel(input_size, hidden_size, output_size).to(self.device)\n",
    "        \n",
    "        # Load state dict with strict=False to ignore missing keys\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Store label mapping\n",
    "        self.label_to_index = checkpoint['label_to_index']\n",
    "        self.index_to_label = {v: k for k, v in self.label_to_index.items()}\n",
    "        \n",
    "        # ResNet feature extractor\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.feature_extractor.to(self.device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        # Image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, video_path):\n",
    "        \"\"\"Extract features from video clips\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        clip_duration = 20  # seconds\n",
    "        clip_frames = int(fps * clip_duration)\n",
    "        \n",
    "        video_features = []\n",
    "        video_clips = []\n",
    "        clip_labels = []\n",
    "        \n",
    "        frame_count = 0\n",
    "        current_clip = []\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            current_clip.append(frame)\n",
    "            frame_count += 1\n",
    "            \n",
    "            # When clip is full or video ends\n",
    "            if len(current_clip) == clip_frames:\n",
    "                # Convert clip to features\n",
    "                clip_tensor = self.process_clip(current_clip)\n",
    "                video_features.append(clip_tensor)\n",
    "                video_clips.append(current_clip)\n",
    "                \n",
    "                # Reset clip\n",
    "                current_clip = []\n",
    "        \n",
    "        # Handle last incomplete clip if exists\n",
    "        if current_clip:\n",
    "            # Pad the clip to match the expected length\n",
    "            while len(current_clip) < clip_frames:\n",
    "                current_clip.append(current_clip[-1])  # Repeat last frame\n",
    "            \n",
    "            clip_tensor = self.process_clip(current_clip)\n",
    "            video_features.append(clip_tensor)\n",
    "            video_clips.append(current_clip)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        return video_features, video_clips\n",
    "\n",
    "    def process_clip(self, clip):\n",
    "        \"\"\"Process a video clip to extract features\"\"\"\n",
    "        clip_features = []\n",
    "        for frame in clip:\n",
    "            # Convert to tensor and extract features\n",
    "            input_tensor = self.transform(frame).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.feature_extractor(input_tensor)\n",
    "                clip_features.append(features.squeeze().cpu().numpy())\n",
    "        \n",
    "        return torch.tensor(clip_features).float().unsqueeze(0)\n",
    "\n",
    "    def predict_highlights(self, video_features):\n",
    "        \"\"\"Predict highlights for each clip\"\"\"\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for features in video_features:\n",
    "                features = features.to(self.device)\n",
    "                output = self.model(features)\n",
    "                prob = F.softmax(output, dim=1)\n",
    "                pred = torch.argmax(prob, dim=1).item()\n",
    "                predicted_label = self.index_to_label[pred]\n",
    "                predictions.append(predicted_label)\n",
    "                print(f\"Predicted label for the clip: {predicted_label}\")  # Print predicted label for each clip\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def create_highlights(self, video_path, highlight_duration_minutes):\n",
    "        \"\"\"\n",
    "        Generate highlights with a specific stitching strategy:\n",
    "        1. Start with a Kick-off clip\n",
    "        2. Distribute Goal clips sequentially throughout the video\n",
    "        3. Add other clips based on ranking\n",
    "        \"\"\"\n",
    "        # Extract features and predict labels\n",
    "        video_features, video_clips = self.extract_features(video_path)\n",
    "        predictions = self.predict_highlights(video_features)\n",
    "        \n",
    "        # Group clips by label\n",
    "        labeled_clips = {}\n",
    "        for label, clip in zip(predictions, video_clips):\n",
    "            if label not in labeled_clips:\n",
    "                labeled_clips[label] = []\n",
    "            labeled_clips[label].append(clip)\n",
    "        \n",
    "        # Sort labels by ranking\n",
    "        sorted_labels = sorted(self.rankings.keys(), key=lambda x: self.rankings[x])\n",
    "        \n",
    "        # Prepare final highlight clips\n",
    "        highlight_clips = []\n",
    "        total_duration = 0\n",
    "        max_duration = highlight_duration_minutes * 60\n",
    "        \n",
    "        # Get video capture details\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        clip_duration = len(video_clips[0]) / fps\n",
    "        cap.release()\n",
    "        \n",
    "        # Strategy for clip selection and sequential placement\n",
    "        \n",
    "        # 1. Start with a Kick-off clip if available\n",
    "        if \"Kick-off\" in labeled_clips and labeled_clips[\"Kick-off\"]:\n",
    "            highlight_clips.append(labeled_clips[\"Kick-off\"].pop(0))\n",
    "            total_duration += clip_duration\n",
    "        \n",
    "        # Prepare Goal clips for sequential insertion\n",
    "        goal_clips = labeled_clips.get(\"Goal\", [])\n",
    "        \n",
    "        # Tracking clip order for goal insertion\n",
    "        label_clip_counts = {label: 0 for label in self.rankings.keys()}\n",
    "        \n",
    "        # Add clips to fill the highlight duration\n",
    "        while total_duration < max_duration:\n",
    "            for label in sorted_labels:\n",
    "                # Skip if we've reached max duration\n",
    "                if total_duration >= max_duration:\n",
    "                    break\n",
    "                \n",
    "                # Special handling for Goal clips\n",
    "                if label == \"Goal\":\n",
    "                    # If goal clips are available, insert them sequentially\n",
    "                    if goal_clips:\n",
    "                        highlight_clips.append(goal_clips.pop(0))\n",
    "                        total_duration += clip_duration\n",
    "                        continue\n",
    "                \n",
    "                # Add clips for other labels\n",
    "                if label in labeled_clips and labeled_clips[label]:\n",
    "                    highlight_clips.append(labeled_clips[label].pop(0))\n",
    "                    total_duration += clip_duration\n",
    "                    label_clip_counts[label] += 1\n",
    "        \n",
    "        # Video writing process\n",
    "        output_path = f'highlights_{highlight_duration_minutes}min.mp4'\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        \n",
    "        # Create video writer\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, \n",
    "                               (highlight_clips[0][0].shape[1], highlight_clips[0][0].shape[0]))\n",
    "        \n",
    "        # Stitch clips together by writing frames\n",
    "        for clip in highlight_clips:\n",
    "            for frame in clip:\n",
    "                out.write(frame)\n",
    "        \n",
    "        out.release()\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "def main():\n",
    "    # Get video path from user\n",
    "    video_path = 'Test_7min.mp4'\n",
    "    \n",
    "    # Validate video path\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"Invalid video path. Please check and try again.\")\n",
    "        return\n",
    "    \n",
    "    # Ask for highlight duration\n",
    "    while True:\n",
    "        try:\n",
    "            duration = int(input(\"How many minutes of highlights do you want? (3/5): \"))\n",
    "            if duration in [3, 5]:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please choose 3 or 5 minutes.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    # Generate highlights\n",
    "    generator = HighlightGenerator()\n",
    "    output_video = generator.create_highlights(video_path, duration)\n",
    "    \n",
    "    print(f\"Highlights generated successfully: {output_video}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
